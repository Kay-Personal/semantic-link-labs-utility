{"cells":[{"cell_type":"markdown","source":["### Install the latest .whl package\n","\n","Check [here](https://pypi.org/project/semantic-link-labs/) to see the latest version."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5c27dfd1-4fe0-4a97-92e6-ddf78889aa93"},{"cell_type":"code","source":["%pip uninstall \"builtin/semantic_link_labs-0.9.4-py3-none-any.whl\" -y"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd5a1a69-8fc3-4fc3-a625-650371506c7d"},{"cell_type":"code","source":["# %pip install semantic-link-labs\n","\n","%pip install \"builtin/semantic_link_labs-0.9.4-py3-none-any.whl\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d5cae9db-cef9-48a8-a351-9c5fcc99645c"},{"cell_type":"markdown","source":["### Requirements\n","* Fabric Capacity with XMLA read/write enabled\n","    * A Fabric Trial Capacity is sufficient for evaluation.\n","    * The XMLA Endpoint must be read/write enabled because the perf lab provisions semantic models automatically.\n","* Fabric Permissions\n","    * User must have permissions to create workspaces, lakehouses, and semantic models. This notebook provisions sample resources to demonstrate the use of a perf lab.\n","    * User should have access to a Fabric capacity. This notebook provisions workspaces, lakehouses, and semantic models on a Fabric capacity.\n","    * Connect this notebook to a lakehouse without a schema to persist test definitions and test results. Although strictly not a requirement, it eliminates the need to provide the name and Id of a disconnected lakehouse.\n","\n","### Result\n","* A master and test workspaces, lakehouses, and semantic models are created to establish a perf lab\n","    * The master workspace contains a lakehouse and a sample semantic model in Direct Lake on OneLake mode that uses the lakehouse as its data source. \n","    * The test workspace contains semantic models cloned from the sample semantic model in the master workspace.\n","    * Various Delta tables are created in the lakehouse connected to this notebook to persist test definitions, table analysis, and test results.\n","    * The resources in the master workspace and in the test workspace are deprovisioned upon completion of the perf lab. Delete the workspaces manually.\n","* The names of the newly created resources can be adjusted to customize the perf lab.\n"],"metadata":{},"id":"2856d26d"},{"cell_type":"markdown","source":["### Import the library and set global notebook parameters\n","\n","This notebook deploys lakehouses and semantic models across different workspaces, but the resources can also be hosted together in a centralized workspace. The master workspace contains a lakehouse with sample data, used as the data source for the sample semantic models in Direct Lake on OneLake mode. The master semantic model serves as a template for the actual test models, which this notebook provisions prior to running the performance tests by cloning the master semantic model."],"metadata":{},"id":"b195eae8"},{"cell_type":"code","source":["\n","import sempy_labs.perf_lab as perf_lab\n","\n","master_workspace = 'Perf Lab Master'                # Enter the name of the master workspace.\n","lakehouse = 'SampleLakehouse'                       # Enter the name of the lakehouse used as the data source.\n","master_dataset = 'Master Semantic Model'            # Enter the name of the master semantic model.\n","\n","test_workspace = 'Perf Lab Testing'                 # Enter the name of the workspace for the semantic model clone.\n","target_dataset_prefix = 'Test Model_'               # Enter the common part of the name for all semantic model clones.\n","test_dataset_A = target_dataset_prefix + 'A'        # Enter the name of the first semantic model clone.\n","test_dataset_B = target_dataset_prefix + 'B'        # Enter the name of the second semantic model clone.\n","\n","capacity_id = None                                  # The Id of the capacity for the workspaces. \n","                                                    # Leave this as None to use the capacity of the attached lakehouse or perf lab notebook.\n","                                        \n","test_definitions_tbl = 'TestDefinitions'            # The name of the table in the notebook-attached lakehouse to store the test definitions.\n","column_segments_tbl = 'StorageTableColumnSegments'  # The name of the table in the notebook-attached lakehouse to store the test definitions.\n","trace_events_tbl = \"TraceEvents\"                    # The name of the table in the notebook-attached lakehouse to store the captured trace events."],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1344e286"},{"cell_type":"markdown","source":["### Working with test definitions\n","\n","Test definitions define the key parameters for the test runs, including the following fields: QueryId, QueryText, MasterWorkspace, MasterDataset, TargetWorkspace, TargetDataset, DatasourceName, DatasourceWorkspace, DatasourceType. The following test code illustrates how to work with the TestDefinition and TestSuite classes.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f634c948-7c23-4d6d-af35-f1f9acdc4dd2"},{"cell_type":"code","source":["first_test_definition = perf_lab.TestDefinition(QueryId=1, QueryText=\"Evaluate {1}\", MasterWorkspace=master_workspace, MasterDataset=master_dataset,\n","                   TargetWorkspace=test_workspace, TargetDataset='Test Model_', DatasourceName=lakehouse,\n","                   DatasourceWorkspace=master_workspace, DatasourceType=\"WrongType\")\n","\n","first_test_definition.remove(\"DatasourceType\")\n","print(first_test_definition.get_keys())\n","first_test_definition.add(\"DatasourceType\", \"Lakehouse\")\n","first_test_definition.TargetDataset='Test Model_A'\n","print(first_test_definition.get_keys())\n","print(first_test_definition.get_values())\n","print(first_test_definition.to_schema())\n","\n","test_definitions = [\n","    first_test_definition,\n","    perf_lab.TestDefinition(QueryId=2, QueryText=\"Evaluate {2}\", MasterWorkspace=master_workspace, MasterDataset=master_dataset,\n","                   TargetWorkspace=test_workspace, TargetDataset='Test Model_B', DatasourceName=lakehouse,\n","                   DatasourceWorkspace=master_workspace, DatasourceType=\"Lakehouse\")\n","]\n","\n","test_suite = perf_lab.TestSuite(test_definitions)\n","test_suite.save_as(test_definitions_tbl)\n","display(test_suite.to_df())\n","\n","test_suite.remove_test_definition(first_test_definition)\n","display(test_suite.to_df())\n","\n","test_suite.clear()\n","test_suite.load(test_definitions_tbl)\n","display(test_suite.to_df())\n","\n","test_suite.add_test_definition(\n","    perf_lab.TestDefinition(QueryId=3, QueryText=\"Evaluate {3}\", MasterWorkspace=master_workspace, MasterDataset=master_dataset,\n","                   TargetWorkspace=test_workspace, TargetDataset='Test Model_C', DatasourceName=lakehouse,\n","                   DatasourceWorkspace=master_workspace, DatasourceType=\"Lakehouse\")\n",")\n","display(test_suite.to_df())\n","\n","test_suite.add_field(\"AdditionalProperty\", \"additional value\")\n","print(test_suite.get_schema())\n","\n","test_suite.remove_field(\"AdditionalProperty\")\n","print(test_suite.get_schema())\n","\n","test_suite.clear()\n","display(test_suite.to_df())\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"651c6feb-810a-4856-91c2-7591916df5d3"},{"cell_type":"markdown","source":["### Run default (incremental), cold, and warm query tests\n","The main purpose of a test run is to measure the performance of a set of DAX queries against the test semantic models with different memory states: Cold (full framing), Semi-warm (incremental framing), and Warm (no framing). Other than running the queries and measuring response times, the run_test_cycle() function must therefore perform additional actions, specifically clearing the cache and refreshing the model."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ecf9fff6-8c02-4e64-9bbc-fe055bcd357b"},{"cell_type":"code","source":["import uuid\n","from datetime import datetime\n","\n","test_workspace = master_workspace\n","test_dataset_A = \"PartitionedSampleMasterModel\"\n","test_suite = perf_lab.TestSuite(\n","    [\n","    perf_lab.TestDefinition(QueryId=\"TestQuery\", QueryText=\"EVALUATE SUMMARIZECOLUMNS(\\\"Test\\\", \\\"Hello World\\\")\", MasterWorkspace=master_workspace, MasterDataset=master_dataset,\n","                   TargetWorkspace=test_workspace, TargetDataset=test_dataset_A, DatasourceName=lakehouse,\n","                   DatasourceWorkspace=master_workspace, DatasourceType=\"Lakehouse\", TestRunId = str(uuid.uuid4()), TestRunTimestamp = str(datetime.now()))\n","    ]\n",")\n","\n","inc_results = perf_lab.run_test_cycle(\n","    test_suite = test_suite,\n","    clear_query_cache = True,\n","    refresh_type = \"clearValuesFull\",\n","    tag = \"for test purposes\"\n","    )\n","\n","display(inc_results[0])\n","display(inc_results[1])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f9e558fd-b9e0-4971-bdec-f4d01a228319"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}